{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cU4YzBrVaU8B",
    "outputId": "7fa5b482-de74-44c7-a00e-8590ceea6ff3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain_community beautifulsoup4 wikipedia arxiv pymupdf youtube-transcript-api python-dotenv langchain-chroma sentence-transformers langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tf-SncpxeXWu",
    "outputId": "29d0a611-911e-47fd-cb9c-fb1187839ab5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.169.0)\n",
      "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.38.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.24.2)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib) (2.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (5.29.4)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xo-3QNrLacEm",
    "outputId": "6e748a9a-0054-435a-faed-9faa50c77371"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader, WikipediaLoader, ArxivLoader, YoutubeLoader\n",
    "import requests\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mpCgXXofkw_O",
    "outputId": "a0a283f8-7897-4414-a0c8-d36a72aee411"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Interactive Learning Assistant!\n",
      "\n",
      "What topic do you want to learn about? LSTM\n",
      "What is your specific learning goal related to this topic? how backpropagation works\n",
      "What is your current level of knowledge on this topic? (e.g., beginner, intermediate, advanced) intermediate\n",
      "How do you prefer to learn? (text, videos, diagrams, examples, all) text\n",
      "\n",
      " Thanks! Here's a summary of your preferences:\n",
      " - Topic: LSTM\n",
      " - Learning objective: how backpropagation works\n",
      " - Prior knowledge: intermediate\n",
      " - Preferred format: text\n"
     ]
    }
   ],
   "source": [
    "def get_learning_preferences():\n",
    "    print(\"Welcome to the Interactive Learning Assistant!\\n\")\n",
    "\n",
    "    topic = input(\"What topic do you want to learn about? \")\n",
    "    objective = input(\"What is your specific learning goal related to this topic? \")\n",
    "    knowledge = input(\"What is your current level of knowledge on this topic? (e.g., beginner, intermediate, advanced) \")\n",
    "    preferred_format = input(\"How do you prefer to learn? (text, videos, diagrams, examples, all) \")\n",
    "\n",
    "    preferences = {\n",
    "        \"topic\": topic.strip(),\n",
    "        \"learning_objective\": objective.strip(),\n",
    "        \"prior_knowledge\": knowledge.strip().lower(),\n",
    "        \"preferred_format\": preferred_format.strip().lower()\n",
    "    }\n",
    "\n",
    "    print(\"\\n Thanks! Here's a summary of your preferences:\")\n",
    "    for k, v in preferences.items():\n",
    "        print(f\" - {k.replace('_', ' ').capitalize()}: {v}\")\n",
    "    return preferences\n",
    "\n",
    "user_preferences = get_learning_preferences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "g82hXTSU3s3g",
    "outputId": "6d6bcc9f-085c-4d82-b81b-37aca5a5b8f2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"I'm a intermediate learner interested in 'LSTM'. My goal is to learn about how backpropagation works, and I prefer learning through text.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preferences_to_query(preferences: dict) -> str:\n",
    "    return (\n",
    "        f\"I'm a {preferences['prior_knowledge']} learner interested in '{preferences['topic']}'. \"\n",
    "        f\"My goal is to learn about {preferences['learning_objective']}, and I prefer learning through {preferences['preferred_format']}.\"\n",
    "    )\n",
    "query = preferences_to_query(user_preferences)\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PAj69h5d-1h9",
    "outputId": "7ef064cf-bcb2-42c8-e6d9-b517a9700461"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM\n"
     ]
    }
   ],
   "source": [
    "topic = user_preferences[\"topic\"]\n",
    "print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RFJ8GhVZKDe"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iuZksEUzacIL",
    "outputId": "dcf7fef2-7800-4f16-803a-17886b5b90a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.11/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WikipediaLoader(\n",
    "    query= topic,\n",
    "    load_max_docs=15,\n",
    "    doc_content_chars_max=10000,\n",
    ")\n",
    "data1 = loader.load()\n",
    "data1_split = text_splitter.split_documents(data1)\n",
    "len(data1_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "dM9x8DM0ZPiE",
    "outputId": "1602fb27-f9f3-485e-baee-ab0be8972c32"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Long short-term memory (LSTM) is a type of recurrent neural network (RNN) aimed at mitigating the vanishing gradient problem commonly encountered by traditional RNNs. Its relative insensitivity to gap length is its advantage over other RNNs, hidden Markov models, and other sequence learning methods. It aims to provide a short-term memory for RNN that can last thousands of timesteps (thus \"long short-term memory\"). The name is made in analogy with long-term memory and short-term memory and their relationship, studied by cognitive psychologists since the early 20th century.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1_split[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ewQlizBbl8Mp",
    "outputId": "dd878ad9-b21c-4e0e-e149-31bb45d92ec2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = ArxivLoader(\n",
    "    query=topic,\n",
    "    load_max_docs=10,\n",
    "    doc_content_chars_max=20000,\n",
    "    load_all_available_meta=False,\n",
    "\n",
    ")\n",
    "data2 = loader.load()\n",
    "data2_split = text_splitter.split_documents(data2)\n",
    "len(data2_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "y6mhZfcS3UuE",
    "outputId": "d0c7ede5-b52b-48a7-986f-bd8f1e63e444"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Spatio-temporal Stacked LSTM for Temperature\\nPrediction in Weather Forecasting\\nZahra Karevan\\nDepartment of Electrical Engineering\\nKU Leuven\\nLeuven, Belgium\\nzahra.karevan@esat.kuleuven.be\\nJohan A. K. Suykens\\nDepartment of Electrical Engineering\\nKU Leuven\\nLeuven, Belgium\\njohan.suykens@esat.kuleuven.be\\nAbstract\\nLong Short-Term Memory (LSTM) is a well-known method used widely on se-\\nquence learning and time series prediction. In this paper we deployed stacked\\nLSTM model in an application of weather forecasting. We propose a 2-layer spatio-\\ntemporal stacked LSTM model which consists of independent LSTM models per\\nlocation in the ﬁrst LSTM layer. Subsequently, the input of the second LSTM\\nlayer is formed based on the combination of the hidden states of the ﬁrst layer\\nLSTM models. The experiments show that by utilizing the spatial information the\\nprediction performance of the stacked LSTM model improves in most of the cases.\\n1\\nIntroduction'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2_split[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZ51As881NAV"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "api_key = userdata.get('YOUTUBE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcroUuPKo-NI",
    "outputId": "418a3a86-71c7-41f7-9b99-da3d27b6db96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.youtube.com/watch?v=b61DPVFX03I',\n",
       " 'https://www.youtube.com/watch?v=YCzL96nL7j0',\n",
       " 'https://www.youtube.com/watch?v=LfnrRPFhkuY',\n",
       " 'https://www.youtube.com/watch?v=8HyCNIVRbSU',\n",
       " 'https://www.youtube.com/watch?v=WCUNPb-5EYI',\n",
       " 'https://www.youtube.com/watch?v=rdkIOM78ZPk',\n",
       " 'https://www.youtube.com/watch?v=P_TZN8kRObQ',\n",
       " 'https://www.youtube.com/watch?v=guqgmVqcy2c',\n",
       " 'https://www.youtube.com/watch?v=clOP2OvogWY',\n",
       " 'https://www.youtube.com/watch?v=eqM9px_wso8']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "SEARCH_QUERY = topic\n",
    "MAX_RESULTS = 10\n",
    "\n",
    "# Changed API_KEY to api_key in the URL\n",
    "url = f'https://www.googleapis.com/youtube/v3/search?part=snippet&type=video&q={SEARCH_QUERY}&maxResults={MAX_RESULTS}&key={api_key}'\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "video_links = [\n",
    "    f\"https://www.youtube.com/watch?v={item['id']['videoId']}\"\n",
    "    for item in data['items']\n",
    "]\n",
    "video_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "klrrNvzVKjdq",
    "outputId": "a6f2774e-9ef5-4f5a-cc9e-fb8441ab52f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping https://www.youtube.com/watch?v=P_TZN8kRObQ due to error: no element found: line 1, column 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3 = []\n",
    "\n",
    "for url in video_links:\n",
    "    try:\n",
    "        loader = YoutubeLoader.from_youtube_url(url, add_video_info=False)\n",
    "        video_data = loader.load()\n",
    "        data3.extend(video_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {url} due to error: {e}\")\n",
    "\n",
    "data3_split = text_splitter.split_documents(data3)\n",
    "len(data3_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "wkYpBEgEM7sO",
    "outputId": "6ef2cc7e-60a6-4fc0-c986-151d4ef1750e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"imagine you're at a murder mystery dinner right at the start the lord of the manor abruptly kills over and your task is to figure out who done it it could be the maid it could be the butler but you've got a problem your short-term memory isn't working so well you can't remember any of the clues past the last 10 minutes well in that sort of situation your prediction is going to be well nothing better than just a random guess or imagine you have the opposite problem where you can remember every word of every conversation that you've ever had if somebody asked you to outline your partner's wedding vows well you might have some trouble doing that there's just so many words that you'd need to process be much better than if you could just remember well the memorable stuff and that's where something called long short term memory comes into play also abbreviated as lstm it allows a neural network to remember the stuff that it needs to keep hold of context but also to forget the stuff that well\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3_split[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424,
     "referenced_widgets": [
      "0de1bbd0fbbb4a7e81b70a0e236d2b9c",
      "54a57db015ec4ac1bf3ebd93bdf58e62",
      "18414633e9c44dc5ac2bc45117e61f48",
      "f8cf4b2f8c9d463d90cd98d2bcc1cb1a",
      "e868de42ac594a30b9abdad9cdc5a9de",
      "b99ccbb9a83146a2aea3a1c4af04987a",
      "69ff7c96906c40429e627ee692c8e396",
      "2543ce1b1754401f9b963210ffeb7696",
      "1b793ca5e3664208938016b9b21badd0",
      "032608d0160145dd901f9fe373724f95",
      "1d9cfb5d216d44e794df15a634576eca",
      "eee35e337e8e4689ac9c4824f35f8ec9",
      "fbe3315db1544b6fa79e3f993f33c06c",
      "4e69eb0e84a74752bb9411b68bcfa1e0",
      "278392e5252d4f1bb8ffdad41856c0c2",
      "80fe8c16c4be4a8f9336c982cbfecb46",
      "2dac8fa066214161949c10cdf1596399",
      "393d26cb5f3c48dd85ca17b4fea93be6",
      "497224949fa44d2fb7fffb6655a1eaf5",
      "d9fd2d55c18f46a2a2d12a8dea839c79",
      "97345bf0cbb94438ac5a29eba93b9ca6",
      "05378a1a34b3437b92815a5da0e5a2b8",
      "a52fb9e1dd874e648963be1cf23f682f",
      "ad66f15c950e4d1d9fafa468cd127d00",
      "f7244baeb91442d2a71dbe22b9eef659",
      "836a5ba21773451ab613995b74f72af0",
      "265d357159884fa38c079c5c3c962053",
      "c3a389f98ee742aca849baa0c7e4e9bc",
      "08921407388146908a1a0ec0d7e971d9",
      "a6a9aa3e064b48b0ad8737a87f7e1f40",
      "3b9496998d604b7888a35f2fe82a7c16",
      "5c603603b1b94e94af65703c81bf5ab1",
      "c8b1148584e7480cafe0abfadb5a55ce",
      "6f44074e4c08408e9e8f29a6b387f577",
      "471cab13e38b4445b4fa47a29ed56ffc",
      "9022ee8c9a714f1d9b12ff5de4254fcc",
      "fd199405dbff4a29804f8d579a143c87",
      "f92b884873314c4f9b26e3e7adccd4ef",
      "f20fefa674ba4100bf345805039ed718",
      "ee22d79a71da45aa87e72854aa67cc4a",
      "8f3957560cb341a0be44eb6d47154ae7",
      "0a9676fbe22b468089a4564caba975a6",
      "ac548c565a4046a4b18fa9f83cf78369",
      "c4bb0a1aa6f94ad0adbc0e6c517a8fb9",
      "26980b18eadd4594b8cb85b9987fb314",
      "3691b082f0af4058857545ce9d8cf8a7",
      "531bb0e4887845cb9319fded02588c43",
      "7e9bbfce8aec4843accf945186c2a69d",
      "d1f13de6ac1d44dcb85e681d71ae50f4",
      "ef692c25ab9841eaa10b8189ca444bba",
      "b5267fdc58214ee3b5b19bc1cd2304d8",
      "e39caa5b1a6b49a3bfc11ad8bca0f716",
      "9b6655dc68f34bfd94cce6cb285fb954",
      "6a776bf6c7404d5792d5e6ced84a0c21",
      "e4497c0d0c914f578ff35db3c92d22f6",
      "af803ee03e904e05994c0221a5611a2a",
      "6a49e3f4e20249979149df78ab672e48",
      "8c467d33575c403387f176ac86ea984b",
      "23ab15fb596e47d58196462533f99a5f",
      "a84d7c3ddbd54fc0a3e3f833a61662e6",
      "f7213712f5024af2954babb536e4d93f",
      "cc3bacdbd1e8465a85a0de26fd23ba49",
      "e0fd076c8fa4446c8a8bc07e5e998523",
      "300d5bad156b41c3a6ee0473af48262d",
      "f0e852cd06b7404f8fd087ec5ba2ba20",
      "3748f53d45254b2bbacc9a8f57e4993c",
      "93eed2753f034f81be99c0d82dae6cb0",
      "cf4577adb0324d8c98a60e9a270f3b81",
      "3d4bfe298a8c4235b826fee94a548ef4",
      "78c605762fcb4322ba50b61e3925b1a8",
      "32c792261ec04da4a423134bb8d08178",
      "80f305ca685a49fc93d3361151e27655",
      "76c5c521571646aeabca10f9989654cf",
      "23908289571c462eb724b0008afc51c8",
      "60412c5ba19c4bf98f9b5d4e1d4ec6d3",
      "f0277cb77c854109a98dbbcaaa1a25cb",
      "0e77d639decc4e37ad1114657fc8ed94",
      "132db5de18b34757b4a51d7f7ea8aaec",
      "5db14448002b42c0b6b303f17874c603",
      "2bc770427db2423ca29ddf86390dcf65",
      "c25c763bfb644073bca6c49bb85fa640",
      "ea81dcdb3855462fa2ca8ebe4530d2e6",
      "57149aed1f0d4ae7a333d661ea60537d",
      "40736591cfad4fe587b2f9db7295c48e",
      "f2458c5d0c064ec3af4e43e003c02323",
      "24a4ba219d724bd88351af8426776c5f",
      "9372538102fd4bc8b4318c1ec49f7a2a",
      "f2b1a6d27dd6428f99e3dd1a60a92086",
      "0235ffdaed4e447ea082d914d82e049b",
      "992478a58a4e41a38ee0a0e257ca875a",
      "661c427384b74f3ca13a844b1c13737f",
      "07f1737330844053a1a9d2ab4dd27e93",
      "6fe3550dc62741cc8178418e6f3a2c8a",
      "1919cde52a6e4588a73328029ded7e7d",
      "d3de491415744f95b74d9bd05c064770",
      "7460819276b64384b3db314b35b275e6",
      "160be37ca24a41a1a339725b926b4fa0",
      "098d9aef66c24b24ba48d52d20f5a9f5",
      "101d3b5e2d8242d4b66c7659e0974ddd",
      "0fd299c7d0a54bc39b61e90aa4dfca61",
      "d22925a9ac304477b12395f268d8b9d7",
      "f1fe593cd1b649a083490fdbc99363bb",
      "3c95a54934254e488e7177a0caca4829",
      "af77389302304d4eb338133cdb71c99d",
      "98d527976c314b5c975eef9d4b58ba36",
      "5bd29bf833d04cbb8dd1d6caa919f864",
      "472e9f1b496b46928a6d4fbfaa15c634",
      "0964e6778cf048508b193c58c2022cf3",
      "d6962d6490ab4a2fbf380e39762ec5f0",
      "ebfc503920b543bc92567a1630f8b2e0",
      "6fef2de6dbbd4dedb3e27b3f53c2a3d5",
      "7322d3cf990e410ca86b0b211b619b32",
      "d5817fa908e04b449587a503d9fb774f",
      "bbfc70949d3f41ffae57775791965414",
      "f1300e55c12f4edc892073e006190e13",
      "809b394a15cd42df84c061968f898552",
      "b80cc27daba34c62a2ddd900998ca2d1",
      "4662d0fbe17e498bbaefcd41f610d4f5",
      "16439ee4b4224a61b85ae030fb0001c3",
      "8c5e4fe5cbde40ac86c9addc16a6cda1",
      "31d2829b0ff9407381758379b214712c"
     ]
    },
    "id": "gQFnuSifSPM1",
    "outputId": "fffd713c-0204-439e-a25e-3165dc5cd251"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-6bfeb1e079e4>:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de1bbd0fbbb4a7e81b70a0e236d2b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee35e337e8e4689ac9c4824f35f8ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52fb9e1dd874e648963be1cf23f682f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f44074e4c08408e9e8f29a6b387f577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26980b18eadd4594b8cb85b9987fb314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af803ee03e904e05994c0221a5611a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93eed2753f034f81be99c0d82dae6cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132db5de18b34757b4a51d7f7ea8aaec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0235ffdaed4e447ea082d914d82e049b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd299c7d0a54bc39b61e90aa4dfca61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fef2de6dbbd4dedb3e27b3f53c2a3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs={\"device\": \"cuda\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHDEQGEpc91L"
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"sample\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5gcdV1Hf_MT"
   },
   "outputs": [],
   "source": [
    "document_ids1 = vector_store.add_documents(documents= data1_split)\n",
    "document_ids2 = vector_store.add_documents(documents= data2_split)\n",
    "document_ids3 = vector_store.add_documents(documents= data3_split)\n",
    "document_ids = document_ids1 + document_ids2 + document_ids3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09-AHOLXh00f",
    "outputId": "5c3bb8d8-fa8e-4f32-a224-c4c838c94f54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqJ7ziSn5PMp"
   },
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\": 40})\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ztXEXgZpG7rY",
    "outputId": "07cd3cc0-a97b-4ae2-ec08-e17e3dfb99ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter API key for Groq: ··········\n",
      "API key set successfully!\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")\n",
    "  print(\"API key set successfully!\")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"llama-3.3-70b-versatile\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ma54nOAqbSK"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an AI educational assistant. Based on the provided context and query, generate a detailed, well-structured educational report in **markdown** format with the following **exact** sections:\n",
    "\n",
    "1. Title\n",
    "Provide a concise, informative title.\n",
    "\n",
    "2. Introduction\n",
    "Introduce the topic, explain its importance.\n",
    "\n",
    "3. Learning Objectives\n",
    "List 3_4 bullet points on what learners will gain.\n",
    "\n",
    "4. Concept Breakdown\n",
    "Explain the core concepts in a logical flow with subheadings if needed.\n",
    "\n",
    "5. Visual Aids Description\n",
    "Describe 2–3 diagrams or figures that would help visualize key concepts.\n",
    "\n",
    "6. Examples and Use Cases\n",
    "Provide real-world examples, applications, or case studies.\n",
    "\n",
    "7. Citations & References\n",
    "Include citations or links to Wikipedia, arXiv, or YouTube that support the content.\n",
    "\n",
    "8. Recommended Additional Resources\n",
    "Suggest 3–5 resources (books, papers, or videos) for further study.\n",
    "\n",
    "9. Conclusion and Next Steps\n",
    "Summarize and recommend what to learn next.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Educational Content:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "final_prompt = prompt.invoke({\"query\": query, \"context\": context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7hTsmjvEMsb0",
    "outputId": "03c69ede-5c70-4803-d613-61817e2eba8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Title\n",
      "Long Short-Term Memory (LSTM) Networks: Understanding the Basics and Applications\n",
      "\n",
      "### Introduction\n",
      "Long Short-Term Memory (LSTM) networks are a type of Recurrent Neural Network (RNN) designed to handle the vanishing gradient problem that occurs in traditional RNNs. This problem arises when the gradients used to update the weights of the network become extremely small, making it difficult for the network to learn long-term dependencies in data. LSTMs have become a crucial component in many state-of-the-art models for tasks such as speech recognition, machine translation, and natural language processing. The importance of LSTMs lies in their ability to learn and remember long-term patterns in sequential data, making them highly effective in applications where understanding the context over extended periods is essential.\n",
      "\n",
      "### Learning Objectives\n",
      "- Understand the basic architecture of LSTM networks, including the cell state, hidden state, forget gate, input gate, and output gate.\n",
      "- Learn how LSTMs overcome the vanishing gradient problem in traditional RNNs.\n",
      "- Familiarize yourself with the applications of LSTMs in real-world problems, such as speech recognition, text classification, and machine translation.\n",
      "- Discover how to implement LSTMs in practice, including data preparation, model training, and evaluation.\n",
      "\n",
      "### Concept Breakdown\n",
      "#### Introduction to RNNs and the Vanishing Gradient Problem\n",
      "RNNs are neural networks designed to handle sequential data. However, they suffer from the vanishing gradient problem during backpropagation, which hinders their ability to learn long-term dependencies. LSTMs were developed to address this issue.\n",
      "\n",
      "#### LSTM Architecture\n",
      "The core components of an LSTM include:\n",
      "- **Cell State (C)**: Acts as the internal memory of the LSTM, allowing it to keep track of information over long periods.\n",
      "- **Hidden State (h)**: Used to capture short-term information and is updated based on the current input and the cell state.\n",
      "- **Forget Gate (f)**: Decides what information from the previous cell state to discard.\n",
      "- **Input Gate (i)**: Determines what new information from the current input to add to the cell state.\n",
      "- **Output Gate (o)**: Controls what information from the cell state to output.\n",
      "\n",
      "#### How LSTMs Work\n",
      "1. **Forget Gate**: The LSTM decides what information to discard from the previous cell state.\n",
      "2. **Input Gate**: The LSTM decides what new information to add to the cell state.\n",
      "3. **Cell State Update**: The cell state is updated based on the decisions from the forget and input gates.\n",
      "4. **Output Gate**: The LSTM decides what information to output based on the updated cell state and hidden state.\n",
      "\n",
      "### Visual Aids Description\n",
      "To better understand LSTMs, several visual aids can be utilized:\n",
      "1. **LSTM Cell Diagram**: Illustrates the components of an LSTM cell, including gates and states.\n",
      "2. **Sequence Processing Diagram**: Shows how LSTMs process sequences of data over time, highlighting the role of each gate.\n",
      "3. **Unrolled LSTM Network**: Visualizes an LSTM network unrolled over time, demonstrating how information flows through the network.\n",
      "\n",
      "### Examples and Use Cases\n",
      "- **Speech Recognition**: LSTMs can learn patterns in speech data to recognize spoken words.\n",
      "- **Text Classification**: LSTMs can classify text into different categories based on the content.\n",
      "- **Machine Translation**: LSTMs are used in sequence-to-sequence models to translate text from one language to another.\n",
      "\n",
      "### Citations & References\n",
      "For further learning, refer to:\n",
      "- [Wikipedia: Long Short-Term Memory](https://en.wikipedia.org/wiki/Long_short-term_memory)\n",
      "- [arXiv: LSTM Paper by Hochreiter and Schmidhuber](https://doi.org/10.1162/neco.1997.9.8.1735)\n",
      "- [YouTube: 3Blue1Brown - LSTM Animation](https://www.youtube.com/watch?v=uj7RM0QT4Ro)\n",
      "\n",
      "### Recommended Additional Resources\n",
      "1. **\"Deep Learning\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville**: A comprehensive textbook covering deep learning techniques, including LSTMs.\n",
      "2. **\"Natural Language Processing (almost) from Scratch\" by Collobert et al.**: A research paper introducing the use of deep learning in NLP tasks, utilizing LSTMs.\n",
      "3. **Stanford CS231n: Convolutional Neural Networks for Visual Recognition**: An online course that covers CNNs but also touches on RNNs and LSTMs for sequence data.\n",
      "\n",
      "### Conclusion and Next Steps\n",
      "In conclusion, LSTMs are powerful tools for handling sequential data and learning long-term dependencies. Their ability to selectively remember and forget information makes them particularly useful in applications like speech recognition, text classification, and machine translation. For further study, delve into the mathematical formulations of LSTMs, explore their applications in different domains, and practice implementing LSTMs using deep learning frameworks like TensorFlow or PyTorch.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(final_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfvWbRHpYg4S"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
